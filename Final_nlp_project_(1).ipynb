{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFGmV4bgyTwa",
        "outputId": "8537f9c7-7792-477f-b3fb-23ef9bcdf6b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH7Zxp7ZyrO_",
        "outputId": "1cb7cba1-3f13-4487-956a-4cf247fd3bf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['combined.jpg', \"Anthoni's Resume.pdf\", 'IMG_20230513_192538.jpg', 'Can kindle read to you.pdf', \"kylieJenner'sDatingHistory_PAMISETTY ANTHONI SAGARIKA.pdf\", 'ANTHONI_SAGARIKA_JS76853.pdf', 'SSR_TSRPT (1).pdf', 'Colab Notebooks', 'APAMISETTY - Signed  Letter of Offer Seasonal Summer (1).pdf', 'email.pdf', 'GA sagarika.pdf', 'Resume_Anthoni_Sagarika.pdf', 'APAMISETTY - Signed  Letter of Offer Seasonal Summer.pdf', 'yelp_academic_dataset_business.csv', 'yelp_academic_dataset_review.csv', 'yelp_academic_dataset_user.csv', 'IBM_Annual_Report_2022.pdf']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "folder_path = '/content/drive/My Drive/'\n",
        "files = os.listdir(folder_path)\n",
        "print(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "785vaexozLYP",
        "outputId": "fb802480-0c14-47ec-c8f5-250611653253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDlbzletzq_1",
        "outputId": "6da0b55e-88af-4199-ec86-98856b29f94c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dwymEt9G0OVw",
        "outputId": "1ea91047-70c8-4bb6-a056-3ba6dec8f46e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycryptodome\n",
            "Successfully installed pycryptodome-3.20.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pycryptodome\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OpEheVyFppQi",
        "outputId": "9062e5a6-679c-4af7-9363-c61db4d50f99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yJbNy46DpxB7",
        "outputId": "4b4f3611-f4c8-4701-f359-cabdcad031a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZNVgFhuzGeh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Extract text from PDF files and perform NLP analysis\n",
        "for file_name in files:\n",
        "    if file_name.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(folder_path, file_name)\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        # Tokenize the text\n",
        "        tokens = word_tokenize(text)\n",
        "        # Remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalnum()]\n",
        "        # Perform word frequency analysis\n",
        "        word_freq = Counter(filtered_tokens)\n",
        "        # Print the most common words\n",
        "        print(f\"Most common words in '{file_name}':\")\n",
        "        print(word_freq.most_common(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zkpOyJR1Hil"
      },
      "outputs": [],
      "source": [
        "# Specify the file name\n",
        "file_name = 'IBM_Annual_Report_2022.pdf'\n",
        "\n",
        "# Construct the full file path\n",
        "pdf_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(pdf_path):\n",
        "    # Extract text from the PDF file\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalnum()]\n",
        "\n",
        "    # Perform word frequency analysis\n",
        "    word_freq = Counter(filtered_tokens)\n",
        "\n",
        "    # Print the most common words\n",
        "    print(f\"Most common words in '{file_name}':\")\n",
        "    print(word_freq.most_common(10))\n",
        "else:\n",
        "    print(f\"File '{file_name}' not found in the specified folder.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGeB7d101OW5"
      },
      "outputs": [],
      "source": [
        "# Check sample output\n",
        "print(len(text))\n",
        "text[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab2XGiM91XIW"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdMZd5iO1V8Y"
      },
      "outputs": [],
      "source": [
        "#help(re.sub)\n",
        "text = re.sub('\\n{1,}', '. ', text)\n",
        "text = re.sub(\"\\s{1,}\", \" \", text)\n",
        "text[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILwm17gs1kyy"
      },
      "outputs": [],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ3YJhG-1fiC"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp_spacy = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8UmBD8i1yP0"
      },
      "outputs": [],
      "source": [
        "doc = nlp_spacy(text)\n",
        "sentences_annual_report = []\n",
        "for sent in doc.sents:\n",
        "    if len(sent.text.split()) > 6:\n",
        "        sentences_annual_report.append(sent.text)\n",
        "\n",
        "f\"Total sentences: {len(sentences_annual_report)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_vQTGG_1522"
      },
      "outputs": [],
      "source": [
        "sentences_annual_report[200:210]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HujtsbEe2JrW"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS8X6ekk1-Wa"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Load FinBert pipeline\n",
        "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbeacDH22oGj"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Define the sentiment analysis pipeline\n",
        "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnSwK1Zs2i_o"
      },
      "outputs": [],
      "source": [
        "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liVHkaiE2aYL"
      },
      "outputs": [],
      "source": [
        "# Examples\n",
        "\n",
        "sentences = [\"there is a shortage of capital, and we need extra financing\",\n",
        "             \"growth is strong and we have plenty of liquidity\",\n",
        "             \"there are doubts about our finances\",\n",
        "             \"profits are flat\"]\n",
        "results = nlp(sentences)\n",
        "\n",
        "for x, y in zip(sentences, results):\n",
        "    print(x)\n",
        "    print(y)\n",
        "    print(\"\\n\")\n",
        "#print(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8SLnZ7c5ie5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnqhYuX925aK"
      },
      "outputs": [],
      "source": [
        "results = nlp(sentences_annual_report)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYsb9d9E5l1g"
      },
      "outputs": [],
      "source": [
        "\n",
        "sentiment = pd.DataFrame({\"docs\": sentences_annual_report,\n",
        "                          \"label\": [r[\"label\"] for r in results],\n",
        "                          \"score\":[r[\"score\"] for r in results]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rmr20nr15pLW"
      },
      "outputs": [],
      "source": [
        "sentiment.label.value_counts().plot(kind=\"barh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_QmUoVy5uFW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Show example sentences for each sentiment label\n",
        "for label in sentiment.label.unique():\n",
        "    print(f\"\\nExamples of sentences for '{label}':\")\n",
        "    example_sentences = sentiment[sentiment.label == label].sample(10)['docs']\n",
        "    for sentence in example_sentences:\n",
        "        print(\"-\", sentence)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS943kvWHKAm"
      },
      "outputs": [],
      "source": [
        "pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRVhPs7mEFnu"
      },
      "outputs": [],
      "source": [
        "from transformers import AlbertForSequenceClassification, AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
        "\n",
        "# For T5 model\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# For T5 model\n",
        "import sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y09ohMOHyBB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load tokenizer and model for summarization\n",
        "sum_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "sum_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Create summarizer pipeline\n",
        "summarizer = pipeline(\"summarization\", tokenizer=sum_tokenizer, model=sum_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iNurCGxInpj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def capitalize_sentences(text):\n",
        "    sentences = text.split('. ')\n",
        "    sentences_capitalized = [sentence.capitalize() for sentence in sentences if sentence]\n",
        "    return '. '.join(sentences_capitalized) + '.'\n",
        "\n",
        "def summarize_text(input_text, min_summary_length=5, max_summary_length=100, model_name=\"facebook/bart-large-cnn\"):\n",
        "    # Load pre-trained model and tokenizer\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize and generate summary\n",
        "    inputs = tokenizer.encode(\"summarize: \" + input_text, return_tensors=\"pt\")\n",
        "    summary_ids = model.generate(inputs, max_length=max_summary_length, min_length=min_summary_length, length_penalty=2.0, num_beams=4)\n",
        "\n",
        "    # Decode and capitalize the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    summary = capitalize_sentences(summary)\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCKfdAQ4U-5_"
      },
      "outputs": [],
      "source": [
        "# Maximum input length for BART-large\n",
        "max_input_length = 1024\n",
        "\n",
        "# Join the sentences from index 1 to 1024\n",
        "input_text = '.'.join(sentences_annual_report[1:1024])\n",
        "\n",
        "# Truncate input text if it exceeds maximum length\n",
        "if len(input_text) > max_input_length:\n",
        "    input_text = input_text[:max_input_length]\n",
        "\n",
        "# Call the summarize_text function with the chosen text\n",
        "summary = summarize_text(input_text)\n",
        "\n",
        "# Print the summary\n",
        "print(\"Summary:\", summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFZQdNvKQtPC"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Create a list containing the summary\n",
        "summaries_list = [summary]\n",
        "\n",
        "# Fit and transform the summaries to TF-IDF vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(summaries_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NszdS5RItMPm"
      },
      "source": [
        "#Name entity recognition:\n",
        "\n",
        "Building a Question answering System, since I love chatbots a lot Iam thinking to implement a best question answering system.\n",
        "\n",
        "\n",
        "Using Hugginface pipeline for QnA with roberta-base-squad2 model (https://huggingface.co/deepset/roberta-base-squad2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Grf91Z6po0Ma"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBJkgNsBlxgk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "# Load model & tokenizer\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create a question-answering pipeline\n",
        "nlp = pipeline('question-answering', model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Ask a question (or press 'q' to quit): \")\n",
        "\n",
        "    if user_input.lower() == 'q':\n",
        "        break\n",
        "\n",
        "    QA_input = {\n",
        "        'question': user_input,\n",
        "        'context': input_text\n",
        "    }\n",
        "\n",
        "    res = nlp(QA_input)\n",
        "    print(res)\n",
        "\n",
        "    print(f\"Answer: {res['answer']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idl1D-CppFvU"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}