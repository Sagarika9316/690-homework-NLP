{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946c0b24",
   "metadata": {},
   "source": [
    "Describe the class of strings matched by the following regular expressions:\n",
    "a. [a-zA-Z]+\n",
    "b. [A-Z][a-z]*\n",
    "c. p[aeiou]{,2}t\n",
    "d. \\d+(\\.\\d+)?\n",
    "e. ([^aeiou][aeiou][^aeiou])*\n",
    "f. \\w+|[^\\w\\s]+\n",
    "Test your answers using nltk.re_show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002cc25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb86a7-c8e2-47e1-b4cf-946405f0e304",
   "metadata": {},
   "source": [
    "Algorithm : \n",
    "\n",
    "Step 1 : Create a sample_strings variable and store the inputs as a list\n",
    "Step 2 : Create a regex_patterns list to store the given regex expressions\n",
    "Step  3 : Create a while loop that iterates in the regex patterns\n",
    "Step 4 : Store the current value of regex in a variable\n",
    "Step 5 : Run a for loop in sample_strings and use nltk.reshow()where it compares and checks if the word is similar to regex\n",
    "Step 6:Print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a31a86",
   "metadata": {},
   "source": [
    "For a  Normal words with more than one word being a upper case or lower case\n",
    "Probably the second regex operation denotes the Title of the chapter where the first letter is Capital\n",
    "This third regex shows the word where it starts with lower case letter \"p\" followed by two vowels from a,e,i,o,u and the word ends with lower \"t\"\n",
    "This regex is a number where it can give decimals,numerator and denominator (Fractions)\n",
    "A consonant where the second letter would be vowel and the third letter is Consonant again\n",
    "Alphanumeric character(s) or non-whitespace character(s), can be used for tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd00f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern: [a-zA-Z]+\n",
      "String: sing\n",
      "{sing}\n",
      "\n",
      "\n",
      "String: Title\n",
      "{Title}\n",
      "\n",
      "\n",
      "String: pet\n",
      "{pet}\n",
      "\n",
      "\n",
      "String: 0.244\n",
      "0.244\n",
      "\n",
      "\n",
      "String: Not\n",
      "{Not}\n",
      "\n",
      "\n",
      "String: pkoJDN\n",
      "{pkoJDN}\n",
      "\n",
      "\n",
      "String: 0LM9\n",
      "0{LM}9\n",
      "\n",
      "\n",
      "Pattern: [A-Z][a-z]*\n",
      "String: sing\n",
      "sing\n",
      "\n",
      "\n",
      "String: Title\n",
      "{Title}\n",
      "\n",
      "\n",
      "String: pet\n",
      "pet\n",
      "\n",
      "\n",
      "String: 0.244\n",
      "0.244\n",
      "\n",
      "\n",
      "String: Not\n",
      "{Not}\n",
      "\n",
      "\n",
      "String: pkoJDN\n",
      "pko{J}{D}{N}\n",
      "\n",
      "\n",
      "String: 0LM9\n",
      "0{L}{M}9\n",
      "\n",
      "\n",
      "Pattern: p[aeiou]{,2}t\n",
      "String: sing\n",
      "sing\n",
      "\n",
      "\n",
      "String: Title\n",
      "Title\n",
      "\n",
      "\n",
      "String: pet\n",
      "{pet}\n",
      "\n",
      "\n",
      "String: 0.244\n",
      "0.244\n",
      "\n",
      "\n",
      "String: Not\n",
      "Not\n",
      "\n",
      "\n",
      "String: pkoJDN\n",
      "pkoJDN\n",
      "\n",
      "\n",
      "String: 0LM9\n",
      "0LM9\n",
      "\n",
      "\n",
      "Pattern: \\d+(\\.\\d+)?\n",
      "String: sing\n",
      "sing\n",
      "\n",
      "\n",
      "String: Title\n",
      "Title\n",
      "\n",
      "\n",
      "String: pet\n",
      "pet\n",
      "\n",
      "\n",
      "String: 0.244\n",
      "{0.244}\n",
      "\n",
      "\n",
      "String: Not\n",
      "Not\n",
      "\n",
      "\n",
      "String: pkoJDN\n",
      "pkoJDN\n",
      "\n",
      "\n",
      "String: 0LM9\n",
      "{0}LM{9}\n",
      "\n",
      "\n",
      "Pattern: ([^aeiou][aeiou][^aeiou])*\n",
      "String: sing\n",
      "{sin}{}g{}\n",
      "\n",
      "\n",
      "String: Title\n",
      "{Tit}{}l{}e{}\n",
      "\n",
      "\n",
      "String: pet\n",
      "{pet}{}\n",
      "\n",
      "\n",
      "String: 0.244\n",
      "{}0{}.{}2{}4{}4{}\n",
      "\n",
      "\n",
      "String: Not\n",
      "{Not}{}\n",
      "\n",
      "\n",
      "String: pkoJDN\n",
      "{}p{koJ}{}D{}N{}\n",
      "\n",
      "\n",
      "String: 0LM9\n",
      "{}0{}L{}M{}9{}\n",
      "\n",
      "\n",
      "Pattern: \\w+|[^\\w\\s]+\n",
      "String: sing\n",
      "{sing}\n",
      "\n",
      "\n",
      "String: Title\n",
      "{Title}\n",
      "\n",
      "\n",
      "String: pet\n",
      "{pet}\n",
      "\n",
      "\n",
      "String: 0.244\n",
      "{0}{.}{244}\n",
      "\n",
      "\n",
      "String: Not\n",
      "{Not}\n",
      "\n",
      "\n",
      "String: pkoJDN\n",
      "{pkoJDN}\n",
      "\n",
      "\n",
      "String: 0LM9\n",
      "{0LM9}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Sample strings for each regex \n",
    "sample_strings= [\"sing\", \"Title\", \"pet\", \"0.244\", \"Not\", \"pkoJDN\", \"0LM9\"]\n",
    "\n",
    "# Regular expressions\n",
    "regex_patterns = [\n",
    "    r'[a-zA-Z]+',\n",
    "    r'[A-Z][a-z]*',\n",
    "    r'p[aeiou]{,2}t',\n",
    "    r'\\d+(\\.\\d+)?',\n",
    "    r'([^aeiou][aeiou][^aeiou])*',\n",
    "    r'\\w+|[^\\w\\s]+'\n",
    "]\n",
    "\n",
    "i = 0\n",
    "\n",
    "# Using a while loop for looping in regex patterns\n",
    "while i < len(regex_patterns):\n",
    "    pattern = regex_patterns[i]\n",
    "    print(\"Pattern:\", pattern)\n",
    "    # Iterate over test strings\n",
    "    for string in sample_strings:\n",
    "        print(\"String:\", string)\n",
    "        nltk.re_show(pattern, string)\n",
    "        print(\"\\n\")\n",
    "    i += 1  # Increment index variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac33e4",
   "metadata": {},
   "source": [
    "Write regular expressions to match the following classes of strings:\n",
    "a. A single determiner (assume that a, an, and the are the only determiners)\n",
    "b. An arithmetic expression using integers, addition, and multiplication, such as\n",
    "2*3+8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9dddec",
   "metadata": {},
   "source": [
    "r'([a][an][the])*',\n",
    "r'\\d+(\\*\\d+)*(\\+\\d+(\\*\\d+)*)*'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f63af047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{the}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Sample string\n",
    "sample_string = \"the\"\n",
    "\n",
    "# Regular expression pattern for a,an,the\n",
    "regex_pattern =   r'\\b(a|an|the)\\b' \n",
    "\n",
    "# Display matches using nltk.re_show()\n",
    "nltk.re_show(regex_pattern, sample_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29915abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2*3+8}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Sample string\n",
    "sample_string = \"2*3+8\"\n",
    "\n",
    "# Regular expression pattern for checking the above string\n",
    "regex_pattern =  r'\\d+(\\*\\d+)*(\\+\\d+(\\*\\d+)*)*' \n",
    "\n",
    "# Display matches using nltk.re_show()\n",
    "nltk.re_show(regex_pattern, sample_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5be2b",
   "metadata": {},
   "source": [
    "â—‘ Write a function unknown() that takes a URL as its argument, and returns a list\n",
    "of unknown words that occur on that web page. In order to do this, extract all\n",
    "substrings consisting of lowercase letters (using re.findall()) and remove any\n",
    "items from this set that occur in the Words Corpus (nltk.corpus.words). Try to\n",
    "categorize these words manually and discuss your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd55cc4-b4bc-4564-8684-fe46436f70d2",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "\n",
    "Step 1 :Import the necessary packages, since it is related to url im using urlopen\n",
    "Step 2 : Use a regex operation to find all the lower case alpha\n",
    "Step 3 : Using set command where it takes the words as set from word corpus\n",
    "Step 4 : Using a list comprehension to identify if the words are not in the word corpus\n",
    "Step 5 : Return the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "265387ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown words on the webpage:\n",
      "['doctype', 'html', 'charset', 'utf', 'docutils', 'http', 'docutils', 'sourceforge', 'viewport', 'nltk', 'toolkit', 'stylesheet', 'href', 'css', 'css', 'stylesheet', 'href', 'css', 'css', 'javascript', 'src', 'js', 'javascript', 'src', 'js', 'javascript', 'src', 'jquery', 'js', 'javascript', 'src', 'js', 'javascript', 'src', 'js', 'javascript', 'src', 'doctools', 'js', 'src', 'https', 'email', 'tl', 'fortawesome', 'com', 'jjps', 'nltk', 'logo', 'href', 'nltk', 'bars', 'javascript', 'toggleclass', 'toggled', 'slidetoggle', 'rtd', 'html', 'placeholder', 'nltk', 'ul', 'toctree', 'href', 'api', 'nltk', 'html', 'api', 'toctree', 'href', 'howto', 'html', 'toctree', 'href', 'py', 'modindex', 'html', 'toctree', 'href', 'https', 'github', 'com', 'nltk', 'nltk', 'wiki', 'wiki', 'toctree', 'href', 'https', 'github', 'com', 'nltk', 'nltk', 'wiki', 'faq', 'faq', 'toctree', 'href', 'https', 'github', 'com', 'nltk', 'nltk', 'issues', 'issues', 'toctree', 'href', 'https', 'github', 'com', 'nltk', 'nltk', 'nltk', 'github', 'ul', 'ul', 'toctree', 'href', 'html', 'installing', 'nltk', 'toctree', 'href', 'html', 'installing', 'nltk', 'ul', 'ul', 'toctree', 'href', 'html', 'notes', 'toctree', 'href', 'html', 'contributing', 'nltk', 'toctree', 'href', 'html', 'nltk', 'ul', 'toolkit', 'toolkit', 'headerlink', 'href', 'toolkit', 'permalink', 'nltk', 'programs', 'provides', 'interfaces', 'href', 'https', 'www', 'nltk', 'org', 'resources', 'wordnet', 'processing', 'libraries', 'tokenization', 'tagging', 'parsing', 'wrappers', 'nlp', 'libraries', 'href', 'https', 'groups', 'google', 'com', 'nltk', 'users', 'hands', 'introducing', 'programming', 'fundamentals', 'topics', 'api', 'nltk', 'linguists', 'engineers', 'students', 'educators', 'researchers', 'users', 'nltk', 'windows', 'linux', 'nltk', 'nltk', 'has', 'called', 'using', 'href', 'https', 'www', 'nltk', 'org', 'processing', 'provides', 'programming', 'processing', 'creators', 'nltk', 'guides', 'fundamentals', 'programs', 'categorizing', 'analyzing', 'online', 'has', 'updated', 'nltk', 'href', 'https', 'www', 'nltk', 'org', 'https', 'www', 'nltk', 'org', 'things', 'nltk', 'things', 'nltk', 'headerlink', 'href', 'things', 'nltk', 'permalink', 'tokenize', 'doctest', 'notranslate', 'pre', 'gp', 'gt', 'gt', 'gt', 'kn', 'nn', 'nltk', 'gp', 'gt', 'gt', 'gt', 'thursday', 'gp', 'arthur', 'didn', 'gp', 'gt', 'gt', 'gt', 'tokens', 'nltk', 'gp', 'gt', 'gt', 'gt', 'tokens', 'thursday', 'arthur', 'gp', 'gt', 'gt', 'gt', 'nltk', 'tokens', 'gp', 'gt', 'gt', 'gt', 'cd', 'jj', 'thursday', 'nnp', 'nn', 'pre', 'named', 'entities', 'doctest', 'notranslate', 'pre', 'gp', 'gt', 'gt', 'gt', 'entities', 'nltk', 'gp', 'gt', 'gt', 'gt', 'entities', 'cd', 'jj', 'thursday', 'nnp', 'nn', 'arthur', 'nnp', 'vbd', 'rb', 'vb', 'rb', 'jj', 'pre', 'doctest', 'notranslate', 'pre', 'gp', 'gt', 'gt', 'gt', 'kn', 'nn', 'nltk', 'kn', 'treebank', 'gp', 'gt', 'gt', 'gt', 'treebank', 'mrg', 'gp', 'gt', 'gt', 'gt', 'pre', 'img', 'src', 'nb', 'uses', 'nltk', 'nltk', 'follows', 'blockquote', 'edward', 'ewan', 'klein', 'processing', 'reilly', 'inc', 'blockquote', 'steps', 'steps', 'headerlink', 'href', 'steps', 'permalink', 'ul', 'href', 'https', 'groups', 'google', 'com', 'nltk', 'announcements', 'href', 'https', 'groups', 'google', 'com', 'nltk', 'users', 'ul', 'toctree', 'toctree', 'toctree', 'info', 'ul', 'details', 'href', 'rst', 'txt', 'nofollow', 'href', 'https', 'github', 'com', 'nltk', 'nltk', 'jan', 'ul', 'nltk', 'created', 'href', 'http', 'org', 'href', 'https', 'github', 'com', 'tomaarsen', 'nltk', 'html']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "#using urllib.request to import url\n",
    "from urllib.request import urlopen\n",
    "from nltk.corpus import words\n",
    "\n",
    "#Creating a function known as unknown\n",
    "def unknown(url,list):\n",
    "    #using urlopen from the documentation and decoding it\n",
    "    raw = urlopen(url).read().decode('utf-8')\n",
    "    \n",
    "   #using re.findall and regex for identifying lower case letters\n",
    "    lowercase_words = re.findall(r'\\b[a-z]+\\b', raw.lower())\n",
    "    \n",
    "    # Remove words that occur in the Words Corpus\n",
    "    words_corpus = set(words.words())\n",
    "    list= [word for word in lowercase_words if word not in words_corpus]\n",
    "    \n",
    "    return list\n",
    "\n",
    "\n",
    "url = \"https://www.nltk.org/\"\n",
    "unknown_words = unknown(url,list)\n",
    "print(\"Unknown words on the webpage:\")\n",
    "print(unknown_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729b9db2-a2ce-417d-97c0-16a7ba9efd7f",
   "metadata": {},
   "source": [
    "Categorizing the word manually and my findings are:\n",
    "\n",
    "I observe some categories of words\n",
    "1)The words which are gibberish\n",
    "2)Some are just a mix of two or three letter repeated words\n",
    "3)Some are disctionary words of two which dont have any space between them\n",
    "4)Some are plurals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc552a",
   "metadata": {},
   "source": [
    "Use the Porter Stemmer to normalize some tokenized text, calling the stemmer\n",
    "on each word. Do the same thing with the Lancaster Stemmer, and see if you observe\n",
    "any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f1990f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4956ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    " raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "... is no basis for a system of government.  Supreme executive power derives from\n",
    "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    ">>> tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5939be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denni',\n",
       " ':',\n",
       " 'listen',\n",
       " ',',\n",
       " 'strang',\n",
       " 'women',\n",
       " 'lie',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandat',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcic',\n",
       " 'aquat',\n",
       " 'ceremoni',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "[porter.stem(t) for t in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d4533b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['den',\n",
       " ':',\n",
       " 'list',\n",
       " ',',\n",
       " 'strange',\n",
       " 'wom',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'bas',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'pow',\n",
       " 'der',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mand',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'som',\n",
       " 'farc',\n",
       " 'aqu',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20ef69-acce-4199-b403-b8f4f2c97782",
   "metadata": {},
   "source": [
    "Both didnt result in the same stemming\n",
    "The lancaster did more aggressive stemming when compared to porter\n",
    "Some examples :\n",
    "dennis : denni,den\n",
    "basis : basis,bas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ee445-12e3-4551-aa80-769a5cba6b53",
   "metadata": {},
   "source": [
    "Algorithm : \n",
    "1)Define a function known as soundex_generator which takes tokens as input\n",
    "2)Covert them to upper case \n",
    "3)Create a dictionary where it has some key value pairs\n",
    "4)Create two for loops to iterate for the tokens and the second for loop for keys in dictionary\n",
    "5)If the key is not equal to \".\" or \"previous key\" then return the soundex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "315392b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T232600\n"
     ]
    }
   ],
   "source": [
    "# Define the function\n",
    "def soundex_generator(token):\n",
    "    token = token.upper()\n",
    "    soundex = \"\"\n",
    "    soundex += token[0]\n",
    "    dictionary = {\n",
    "        \"BFPV\": \"1\",\n",
    "        \"CGJKQSXZ\": \"2\",\n",
    "        \"DT\": \"3\",\n",
    "        \"L\": \"4\",\n",
    "        \"MN\": \"5\",\n",
    "        \"R\": \"6\",\n",
    "        \"AEIOUHWY\": \".\",\n",
    "    }\n",
    "    for char in token[1:]:\n",
    "        for key in dictionary.keys():\n",
    "            if char in key:\n",
    "                code = dictionary[key]\n",
    "                if code != \".\":\n",
    "                    if code != soundex[-1]:\n",
    "                        soundex += code\n",
    "    soundex = soundex[:7].ljust(7, \"0\")\n",
    "    return soundex\n",
    "\n",
    "print(soundex_generator(\"The code is here\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eeab89",
   "metadata": {},
   "source": [
    "â—‘ Write a function novel10(text) that prints any word that appeared in the last\n",
    "10% of a text that had not been encountered earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0fbd2-7cd8-44c2-a149-41b8d4c92da2",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "1)Initialize a function novel10\n",
    "2)Initialize sets where it stores the words\n",
    "3)Do a for loop where it operates on the 10 % of the last text\n",
    "4)If the word is not the last 10 % of the text then add the words into the set\n",
    "5)Just print the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21cfcaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "released,\n",
      "problem\n",
      "weeks\n",
      "some\n",
      "detected\n"
     ]
    }
   ],
   "source": [
    "def novel10(text):\n",
    "    length = len(text)\n",
    "    #initializing the set of words to store \n",
    "    sets = set()\n",
    "    \n",
    "    for word in text[int(0.9 * length):]:\n",
    "        if word not in text[:int(0.9 * length)]:\n",
    "            sets.add(word)\n",
    "    for word in sets:\n",
    "        print(word)\n",
    "\n",
    "# Example usage:\n",
    "text = \"synset names have the form tree.n.01, with three components separated using periods. The NLTK WordNet module initially decomposed these names using split('.'). However, this method broke when someone tried to look up the word PhD, which has the synset name ph.d..n.01, containing four periods instead of the expected two. The solution was to use rsplit('.', 2) to do at most two splits, using the rightmost instances of the period, and leaving the ph.d. string intact. Although several people had tested the module before it was released, it was some weeks before someone detected the problem\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "text_words = text.split()\n",
    "\n",
    "# Call the novel10 function\n",
    "novel10(text_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c98c7fa",
   "metadata": {},
   "source": [
    "Write a function that takes a list of words (containing duplicates) and returns a\n",
    "list of words (with no duplicates) sorted by decreasing frequency. E.g., if the input\n",
    "list contained 10 instances of the word table and 9 instances of the word chair,\n",
    "then table would appear before chair in the output list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c0d0e-dfc2-4737-881c-0a919e11dc58",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "Step 1 : Import necessary functions\n",
    "Step 2 : Create a function then use nltk.word_tokenize to help splitting of words into tokens\n",
    "Step 3: If the word is in lower case and count the words using freqdist from nltk \n",
    "Step 4: Return the words in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5fa3a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'functions', 'more', 'to', 'with', 'appropriate', 'use', 'makes', 'programs', 'readable', 'and', 'maintainable.additionally', 'it', 'becomes', 'possible', 'reimplement', 'a', 'functionâ€”replacing', 'body', 'efficient', 'codeâ€”without', 'having', 'be', 'concerned', 'rest', 'program']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "def novels20(text):\n",
    "    freqdist = FreqDist()\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        freqdist[word.lower()] += 1\n",
    "    common_words = freqdist.most_common() \n",
    "    common_words = [word for word, _ in common_words] \n",
    "    return common_words\n",
    "\n",
    "# Sample text\n",
    "text = \"Appropriate use of functions makes programs more readable and maintainable.Additionally it becomes possible to reimplement a functionâ€”replacing the functions body with more efficient codeâ€”without having to be concerned with the rest of the program\"\n",
    "\n",
    "common_words = novels20(text)\n",
    "print(common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d803da",
   "metadata": {},
   "source": [
    "â—‘ Read about string edit distance and the Levenshtein Algorithm. Try the implementation\n",
    "provided in nltk.edit_dist(). In what way is this using dynamic programming?\n",
    "Does it use the bottom-up or top-down approach? (See also http://\n",
    "norvig.com/spell-correct.html.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed90480",
   "metadata": {},
   "source": [
    "The following code has been taken from https://www.nltk.org/_modules/nltk/metrics/distance.html for observing the implementation as the provided address is throwing a 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1452bfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance btwn 'rain' and 'shine': 3\n",
      "Edit dist with transpositions btwn 'rain' and 'shine': 3\n",
      "Jaro similarity btwn 'rain' and 'shine': 0.6333333333333333\n",
      "Jaro-Winkler similarity btwn 'rain' and 'shine': 0.6333333333333333\n",
      "Jaro-Winkler distance btwn 'rain' and 'shine': 0.3666666666666667\n",
      "Edit distance btwn 'abcdef' and 'acbdef': 2\n",
      "Edit dist with transpositions btwn 'abcdef' and 'acbdef': 1\n",
      "Jaro similarity btwn 'abcdef' and 'acbdef': 0.9444444444444444\n",
      "Jaro-Winkler similarity btwn 'abcdef' and 'acbdef': 0.95\n",
      "Jaro-Winkler distance btwn 'abcdef' and 'acbdef': 0.050000000000000044\n",
      "Edit distance btwn 'language' and 'lnaguaeg': 4\n",
      "Edit dist with transpositions btwn 'language' and 'lnaguaeg': 2\n",
      "Jaro similarity btwn 'language' and 'lnaguaeg': 0.9166666666666666\n",
      "Jaro-Winkler similarity btwn 'language' and 'lnaguaeg': 0.9249999999999999\n",
      "Jaro-Winkler distance btwn 'language' and 'lnaguaeg': 0.07500000000000007\n",
      "Edit distance btwn 'language' and 'lnaugage': 3\n",
      "Edit dist with transpositions btwn 'language' and 'lnaugage': 2\n",
      "Jaro similarity btwn 'language' and 'lnaugage': 0.9166666666666666\n",
      "Jaro-Winkler similarity btwn 'language' and 'lnaugage': 0.9249999999999999\n",
      "Jaro-Winkler distance btwn 'language' and 'lnaugage': 0.07500000000000007\n",
      "Edit distance btwn 'language' and 'lngauage': 2\n",
      "Edit dist with transpositions btwn 'language' and 'lngauage': 2\n",
      "Jaro similarity btwn 'language' and 'lngauage': 0.9583333333333333\n",
      "Jaro-Winkler similarity btwn 'language' and 'lngauage': 0.9624999999999999\n",
      "Jaro-Winkler distance btwn 'language' and 'lngauage': 0.03750000000000009\n",
      "s1: {1, 2, 3, 4}\n",
      "s2: {3, 4, 5}\n",
      "Binary distance: 1.0\n",
      "Jaccard distance: 0.6\n",
      "MASI distance: 0.868\n"
     ]
    }
   ],
   "source": [
    "# Natural Language Toolkit: Distance Metrics\n",
    "#\n",
    "# Copyright (C) 2001-2023 NLTK Project\n",
    "# Author: Edward Loper <edloper@gmail.com>\n",
    "#         Steven Bird <stevenbird1@gmail.com>\n",
    "#         Tom Lippincott <tom@cs.columbia.edu>\n",
    "# URL: <https://www.nltk.org/>\n",
    "# For license information, see LICENSE.TXT\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "Distance Metrics.\n",
    "\n",
    "Compute the distance between two items (usually strings).\n",
    "As metrics, they must satisfy the following three requirements:\n",
    "\n",
    "1. d(a, a) = 0\n",
    "2. d(a, b) >= 0\n",
    "3. d(a, c) <= d(a, b) + d(b, c)\n",
    "\"\"\"\n",
    "\n",
    "import operator\n",
    "import warnings\n",
    "\n",
    "\n",
    "def _edit_dist_init(len1, len2):\n",
    "    lev = []\n",
    "    for i in range(len1):\n",
    "        lev.append([0] * len2)  # initialize 2D array to zero\n",
    "    for i in range(len1):\n",
    "        lev[i][0] = i  # column 0: 0,1,2,3,4,...\n",
    "    for j in range(len2):\n",
    "        lev[0][j] = j  # row 0: 0,1,2,3,4,...\n",
    "    return lev\n",
    "\n",
    "\n",
    "def _last_left_t_init(sigma):\n",
    "    return {c: 0 for c in sigma}\n",
    "\n",
    "\n",
    "def _edit_dist_step(\n",
    "    lev, i, j, s1, s2, last_left, last_right, substitution_cost=1, transpositions=False\n",
    "):\n",
    "    c1 = s1[i - 1]\n",
    "    c2 = s2[j - 1]\n",
    "\n",
    "    # skipping a character in s1\n",
    "    a = lev[i - 1][j] + 1\n",
    "    # skipping a character in s2\n",
    "    b = lev[i][j - 1] + 1\n",
    "    # substitution\n",
    "    c = lev[i - 1][j - 1] + (substitution_cost if c1 != c2 else 0)\n",
    "\n",
    "    # transposition\n",
    "    d = c + 1  # never picked by default\n",
    "    if transpositions and last_left > 0 and last_right > 0:\n",
    "        d = lev[last_left - 1][last_right - 1] + i - last_left + j - last_right - 1\n",
    "\n",
    "    # pick the cheapest\n",
    "    lev[i][j] = min(a, b, c, d)\n",
    "\n",
    "\n",
    "def edit_distance(s1, s2, substitution_cost=1, transpositions=False):\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein edit-distance between two strings.\n",
    "    The edit distance is the number of characters that need to be\n",
    "    substituted, inserted, or deleted, to transform s1 into s2.  For\n",
    "    example, transforming \"rain\" to \"shine\" requires three steps,\n",
    "    consisting of two substitutions and one insertion:\n",
    "    \"rain\" -> \"sain\" -> \"shin\" -> \"shine\".  These operations could have\n",
    "    been done in other orders, but at least three steps are needed.\n",
    "\n",
    "    Allows specifying the cost of substitution edits (e.g., \"a\" -> \"b\"),\n",
    "    because sometimes it makes sense to assign greater penalties to\n",
    "    substitutions.\n",
    "\n",
    "    This also optionally allows transposition edits (e.g., \"ab\" -> \"ba\"),\n",
    "    though this is disabled by default.\n",
    "\n",
    "    :param s1, s2: The strings to be analysed\n",
    "    :param transpositions: Whether to allow transposition edits\n",
    "    :type s1: str\n",
    "    :type s2: str\n",
    "    :type substitution_cost: int\n",
    "    :type transpositions: bool\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    # set up a 2-D array\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "    lev = _edit_dist_init(len1 + 1, len2 + 1)\n",
    "\n",
    "    # retrieve alphabet\n",
    "    sigma = set()\n",
    "    sigma.update(s1)\n",
    "    sigma.update(s2)\n",
    "\n",
    "    # set up table to remember positions of last seen occurrence in s1\n",
    "    last_left_t = _last_left_t_init(sigma)\n",
    "\n",
    "    # iterate over the array\n",
    "    # i and j start from 1 and not 0 to stay close to the wikipedia pseudo-code\n",
    "    # see https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance\n",
    "    for i in range(1, len1 + 1):\n",
    "        last_right_buf = 0\n",
    "        for j in range(1, len2 + 1):\n",
    "            last_left = last_left_t[s2[j - 1]]\n",
    "            last_right = last_right_buf\n",
    "            if s1[i - 1] == s2[j - 1]:\n",
    "                last_right_buf = j\n",
    "            _edit_dist_step(\n",
    "                lev,\n",
    "                i,\n",
    "                j,\n",
    "                s1,\n",
    "                s2,\n",
    "                last_left,\n",
    "                last_right,\n",
    "                substitution_cost=substitution_cost,\n",
    "                transpositions=transpositions,\n",
    "            )\n",
    "        last_left_t[s1[i - 1]] = i\n",
    "    return lev[len1][len2]\n",
    "\n",
    "\n",
    "\n",
    "def _edit_dist_backtrace(lev):\n",
    "    i, j = len(lev) - 1, len(lev[0]) - 1\n",
    "    alignment = [(i, j)]\n",
    "\n",
    "    while (i, j) != (0, 0):\n",
    "        directions = [\n",
    "            (i - 1, j - 1),  # substitution\n",
    "            (i - 1, j),  # skip s1\n",
    "            (i, j - 1),  # skip s2\n",
    "        ]\n",
    "\n",
    "        direction_costs = (\n",
    "            (lev[i][j] if (i >= 0 and j >= 0) else float(\"inf\"), (i, j))\n",
    "            for i, j in directions\n",
    "        )\n",
    "        _, (i, j) = min(direction_costs, key=operator.itemgetter(0))\n",
    "\n",
    "        alignment.append((i, j))\n",
    "    return list(reversed(alignment))\n",
    "\n",
    "\n",
    "def edit_distance_align(s1, s2, substitution_cost=1):\n",
    "    \"\"\"\n",
    "    Calculate the minimum Levenshtein edit-distance based alignment\n",
    "    mapping between two strings. The alignment finds the mapping\n",
    "    from string s1 to s2 that minimizes the edit distance cost.\n",
    "    For example, mapping \"rain\" to \"shine\" would involve 2\n",
    "    substitutions, 2 matches and an insertion resulting in\n",
    "    the following mapping:\n",
    "    [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (4, 5)]\n",
    "    NB: (0, 0) is the start state without any letters associated\n",
    "    See more: https://web.stanford.edu/class/cs124/lec/med.pdf\n",
    "\n",
    "    In case of multiple valid minimum-distance alignments, the\n",
    "    backtrace has the following operation precedence:\n",
    "\n",
    "    1. Substitute s1 and s2 characters\n",
    "    2. Skip s1 character\n",
    "    3. Skip s2 character\n",
    "\n",
    "    The backtrace is carried out in reverse string order.\n",
    "\n",
    "    This function does not support transposition.\n",
    "\n",
    "    :param s1, s2: The strings to be aligned\n",
    "    :type s1: str\n",
    "    :type s2: str\n",
    "    :type substitution_cost: int\n",
    "    :rtype: List[Tuple(int, int)]\n",
    "    \"\"\"\n",
    "    # set up a 2-D array\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "    lev = _edit_dist_init(len1 + 1, len2 + 1)\n",
    "\n",
    "    # iterate over the array\n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            _edit_dist_step(\n",
    "                lev,\n",
    "                i + 1,\n",
    "                j + 1,\n",
    "                s1,\n",
    "                s2,\n",
    "                0,\n",
    "                0,\n",
    "                substitution_cost=substitution_cost,\n",
    "                transpositions=False,\n",
    "            )\n",
    "\n",
    "    # backtrace to find alignment\n",
    "    alignment = _edit_dist_backtrace(lev)\n",
    "    return alignment\n",
    "\n",
    "\n",
    "def binary_distance(label1, label2):\n",
    "    \"\"\"Simple equality test.\n",
    "\n",
    "    0.0 if the labels are identical, 1.0 if they are different.\n",
    "\n",
    "    >>> from nltk.metrics import binary_distance\n",
    "    >>> binary_distance(1,1)\n",
    "    0.0\n",
    "\n",
    "    >>> binary_distance(1,3)\n",
    "    1.0\n",
    "    \"\"\"\n",
    "\n",
    "    return 0.0 if label1 == label2 else 1.0\n",
    "\n",
    "def jaccard_distance(label1, label2):\n",
    "    \"\"\"Distance metric comparing set-similarity.\"\"\"\n",
    "    return (len(label1.union(label2)) - len(label1.intersection(label2))) / len(\n",
    "        label1.union(label2)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def masi_distance(label1, label2):\n",
    "    \"\"\"Distance metric that takes into account partial agreement when multiple\n",
    "    labels are assigned.\n",
    "\n",
    "    >>> from nltk.metrics import masi_distance\n",
    "    >>> masi_distance(set([1, 2]), set([1, 2, 3, 4]))\n",
    "    0.665\n",
    "\n",
    "    Passonneau 2006, Measuring Agreement on Set-Valued Items (MASI)\n",
    "    for Semantic and Pragmatic Annotation.\n",
    "    \"\"\"\n",
    "\n",
    "    len_intersection = len(label1.intersection(label2))\n",
    "    len_union = len(label1.union(label2))\n",
    "    len_label1 = len(label1)\n",
    "    len_label2 = len(label2)\n",
    "    if len_label1 == len_label2 and len_label1 == len_intersection:\n",
    "        m = 1\n",
    "    elif len_intersection == min(len_label1, len_label2):\n",
    "        m = 0.67\n",
    "    elif len_intersection > 0:\n",
    "        m = 0.33\n",
    "    else:\n",
    "        m = 0\n",
    "\n",
    "    return 1 - len_intersection / len_union * m\n",
    "\n",
    "\n",
    "\n",
    "def interval_distance(label1, label2):\n",
    "    \"\"\"Krippendorff's interval distance metric\n",
    "\n",
    "    >>> from nltk.metrics import interval_distance\n",
    "    >>> interval_distance(1,10)\n",
    "    81\n",
    "\n",
    "    Krippendorff 1980, Content Analysis: An Introduction to its Methodology\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return pow(label1 - label2, 2)\n",
    "    #        return pow(list(label1)[0]-list(label2)[0],2)\n",
    "    except:\n",
    "        print(\"non-numeric labels not supported with interval distance\")\n",
    "\n",
    "\n",
    "\n",
    "def presence(label):\n",
    "    \"\"\"Higher-order function to test presence of a given label\"\"\"\n",
    "\n",
    "    return lambda x, y: 1.0 * ((label in x) == (label in y))\n",
    "\n",
    "\n",
    "def fractional_presence(label):\n",
    "    return (\n",
    "        lambda x, y: abs((1.0 / len(x)) - (1.0 / len(y))) * (label in x and label in y)\n",
    "        or 0.0 * (label not in x and label not in y)\n",
    "        or abs(1.0 / len(x)) * (label in x and label not in y)\n",
    "        or (1.0 / len(y)) * (label not in x and label in y)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def custom_distance(file):\n",
    "    data = {}\n",
    "    with open(file) as infile:\n",
    "        for l in infile:\n",
    "            labelA, labelB, dist = l.strip().split(\"\\t\")\n",
    "            labelA = frozenset([labelA])\n",
    "            labelB = frozenset([labelB])\n",
    "            data[frozenset([labelA, labelB])] = float(dist)\n",
    "    return lambda x, y: data[frozenset([x, y])]\n",
    "\n",
    "\n",
    "\n",
    "def jaro_similarity(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Jaro similarity between 2 sequences from:\n",
    "\n",
    "        Matthew A. Jaro (1989). Advances in record linkage methodology\n",
    "        as applied to the 1985 census of Tampa Florida. Journal of the\n",
    "        American Statistical Association. 84 (406): 414-20.\n",
    "\n",
    "    The Jaro distance between is the min no. of single-character transpositions\n",
    "    required to change one word into another. The Jaro similarity formula from\n",
    "    https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance :\n",
    "\n",
    "        ``jaro_sim = 0 if m = 0 else 1/3 * (m/|s_1| + m/s_2 + (m-t)/m)``\n",
    "\n",
    "    where\n",
    "        - `|s_i|` is the length of string `s_i`\n",
    "        - `m` is the no. of matching characters\n",
    "        - `t` is the half no. of possible transpositions.\n",
    "    \"\"\"\n",
    "    # First, store the length of the strings\n",
    "    # because they will be re-used several times.\n",
    "    len_s1, len_s2 = len(s1), len(s2)\n",
    "\n",
    "    # The upper bound of the distance for being a matched character.\n",
    "    match_bound = max(len_s1, len_s2) // 2 - 1\n",
    "\n",
    "    # Initialize the counts for matches and transpositions.\n",
    "    matches = 0  # no.of matched characters in s1 and s2\n",
    "    transpositions = 0  # no. of transpositions between s1 and s2\n",
    "    flagged_1 = []  # positions in s1 which are matches to some character in s2\n",
    "    flagged_2 = []  # positions in s2 which are matches to some character in s1\n",
    "\n",
    "    # Iterate through sequences, check for matches and compute transpositions.\n",
    "    for i in range(len_s1):  # Iterate through each character.\n",
    "        upperbound = min(i + match_bound, len_s2 - 1)\n",
    "        lowerbound = max(0, i - match_bound)\n",
    "        for j in range(lowerbound, upperbound + 1):\n",
    "            if s1[i] == s2[j] and j not in flagged_2:\n",
    "                matches += 1\n",
    "                flagged_1.append(i)\n",
    "                flagged_2.append(j)\n",
    "                break\n",
    "    flagged_2.sort()\n",
    "    for i, j in zip(flagged_1, flagged_2):\n",
    "        if s1[i] != s2[j]:\n",
    "            transpositions += 1\n",
    "\n",
    "    if matches == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (\n",
    "            1\n",
    "            / 3\n",
    "            * (\n",
    "                matches / len_s1\n",
    "                + matches / len_s2\n",
    "                + (matches - transpositions // 2) / matches\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def jaro_winkler_similarity(s1, s2, p=0.1, max_l=4):\n",
    "    \"\"\"\n",
    "    The Jaro Winkler distance is an extension of the Jaro similarity in:\n",
    "\n",
    "        William E. Winkler. 1990. String Comparator Metrics and Enhanced\n",
    "        Decision Rules in the Fellegi-Sunter Model of Record Linkage.\n",
    "        Proceedings of the Section on Survey Research Methods.\n",
    "        American Statistical Association: 354-359.\n",
    "\n",
    "    such that:\n",
    "\n",
    "        jaro_winkler_sim = jaro_sim + ( l * p * (1 - jaro_sim) )\n",
    "\n",
    "    where,\n",
    "\n",
    "    - jaro_sim is the output from the Jaro Similarity,\n",
    "        see jaro_similarity()\n",
    "    - l is the length of common prefix at the start of the string\n",
    "        - this implementation provides an upperbound for the l value\n",
    "            to keep the prefixes.A common value of this upperbound is 4.\n",
    "    - p is the constant scaling factor to overweigh common prefixes.\n",
    "        The Jaro-Winkler similarity will fall within the [0, 1] bound,\n",
    "        given that max(p)<=0.25 , default is p=0.1 in Winkler (1990)\n",
    "\n",
    "\n",
    "    Test using outputs from https://www.census.gov/srd/papers/pdf/rr93-8.pdf\n",
    "    from \"Table 5 Comparison of String Comparators Rescaled between 0 and 1\"\n",
    "\n",
    "    >>> winkler_examples = [(\"billy\", \"billy\"), (\"billy\", \"bill\"), (\"billy\", \"blily\"),\n",
    "    ... (\"massie\", \"massey\"), (\"yvette\", \"yevett\"), (\"billy\", \"bolly\"), (\"dwayne\", \"duane\"),\n",
    "    ... (\"dixon\", \"dickson\"), (\"billy\", \"susan\")]\n",
    "\n",
    "    >>> winkler_scores = [1.000, 0.967, 0.947, 0.944, 0.911, 0.893, 0.858, 0.853, 0.000]\n",
    "    >>> jaro_scores =    [1.000, 0.933, 0.933, 0.889, 0.889, 0.867, 0.822, 0.790, 0.000]\n",
    "\n",
    "    One way to match the values on the Winkler's paper is to provide a different\n",
    "    p scaling factor for different pairs of strings, e.g.\n",
    "\n",
    "    >>> p_factors = [0.1, 0.125, 0.20, 0.125, 0.20, 0.20, 0.20, 0.15, 0.1]\n",
    "\n",
    "    >>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):\n",
    "    ...     assert round(jaro_similarity(s1, s2), 3) == jscore\n",
    "    ...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore\n",
    "\n",
    "\n",
    "    Test using outputs from https://www.census.gov/srd/papers/pdf/rr94-5.pdf from\n",
    "    \"Table 2.1. Comparison of String Comparators Using Last Names, First Names, and Street Names\"\n",
    "\n",
    "    >>> winkler_examples = [('SHACKLEFORD', 'SHACKELFORD'), ('DUNNINGHAM', 'CUNNIGHAM'),\n",
    "    ... ('NICHLESON', 'NICHULSON'), ('JONES', 'JOHNSON'), ('MASSEY', 'MASSIE'),\n",
    "    ... ('ABROMS', 'ABRAMS'), ('HARDIN', 'MARTINEZ'), ('ITMAN', 'SMITH'),\n",
    "    ... ('JERALDINE', 'GERALDINE'), ('MARHTA', 'MARTHA'), ('MICHELLE', 'MICHAEL'),\n",
    "    ... ('JULIES', 'JULIUS'), ('TANYA', 'TONYA'), ('DWAYNE', 'DUANE'), ('SEAN', 'SUSAN'),\n",
    "    ... ('JON', 'JOHN'), ('JON', 'JAN'), ('BROOKHAVEN', 'BRROKHAVEN'),\n",
    "    ... ('BROOK HALLOW', 'BROOK HLLW'), ('DECATUR', 'DECATIR'), ('FITZRUREITER', 'FITZENREITER'),\n",
    "    ... ('HIGBEE', 'HIGHEE'), ('HIGBEE', 'HIGVEE'), ('LACURA', 'LOCURA'), ('IOWA', 'IONA'), ('1ST', 'IST')]\n",
    "\n",
    "    >>> jaro_scores =   [0.970, 0.896, 0.926, 0.790, 0.889, 0.889, 0.722, 0.467, 0.926,\n",
    "    ... 0.944, 0.869, 0.889, 0.867, 0.822, 0.783, 0.917, 0.000, 0.933, 0.944, 0.905,\n",
    "    ... 0.856, 0.889, 0.889, 0.889, 0.833, 0.000]\n",
    "\n",
    "    >>> winkler_scores = [0.982, 0.896, 0.956, 0.832, 0.944, 0.922, 0.722, 0.467, 0.926,\n",
    "    ... 0.961, 0.921, 0.933, 0.880, 0.858, 0.805, 0.933, 0.000, 0.947, 0.967, 0.943,\n",
    "    ... 0.913, 0.922, 0.922, 0.900, 0.867, 0.000]\n",
    "\n",
    "    One way to match the values on the Winkler's paper is to provide a different\n",
    "    p scaling factor for different pairs of strings, e.g.\n",
    "\n",
    "    >>> p_factors = [0.1, 0.1, 0.1, 0.1, 0.125, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.20,\n",
    "    ... 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "\n",
    "\n",
    "    >>> for (s1, s2), jscore, wscore, p in zip(winkler_examples, jaro_scores, winkler_scores, p_factors):\n",
    "    ...     if (s1, s2) in [('JON', 'JAN'), ('1ST', 'IST')]:\n",
    "    ...         continue  # Skip bad examples from the paper.\n",
    "    ...     assert round(jaro_similarity(s1, s2), 3) == jscore\n",
    "    ...     assert round(jaro_winkler_similarity(s1, s2, p=p), 3) == wscore\n",
    "\n",
    "\n",
    "\n",
    "    This test-case proves that the output of Jaro-Winkler similarity depends on\n",
    "    the product  l * p and not on the product max_l * p. Here the product max_l * p > 1\n",
    "    however the product l * p <= 1\n",
    "\n",
    "    >>> round(jaro_winkler_similarity('TANYA', 'TONYA', p=0.1, max_l=100), 3)\n",
    "    0.88\n",
    "    \"\"\"\n",
    "    # To ensure that the output of the Jaro-Winkler's similarity\n",
    "    # falls between [0,1], the product of l * p needs to be\n",
    "    # also fall between [0,1].\n",
    "    if not 0 <= max_l * p <= 1:\n",
    "        warnings.warn(\n",
    "            str(\n",
    "                \"The product  `max_l * p` might not fall between [0,1].\"\n",
    "                \"Jaro-Winkler similarity might not be between 0 and 1.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Compute the Jaro similarity\n",
    "    jaro_sim = jaro_similarity(s1, s2)\n",
    "\n",
    "    # Initialize the upper bound for the no. of prefixes.\n",
    "    # if user did not pre-define the upperbound,\n",
    "    # use shorter length between s1 and s2\n",
    "\n",
    "    # Compute the prefix matches.\n",
    "    l = 0\n",
    "    # zip() will automatically loop until the end of shorter string.\n",
    "    for s1_i, s2_i in zip(s1, s2):\n",
    "        if s1_i == s2_i:\n",
    "            l += 1\n",
    "        else:\n",
    "            break\n",
    "        if l == max_l:\n",
    "            break\n",
    "    # Return the similarity value as described in docstring.\n",
    "    return jaro_sim + (l * p * (1 - jaro_sim))\n",
    "\n",
    "\n",
    "\n",
    "def demo():\n",
    "    string_distance_examples = [\n",
    "        (\"rain\", \"shine\"),\n",
    "        (\"abcdef\", \"acbdef\"),\n",
    "        (\"language\", \"lnaguaeg\"),\n",
    "        (\"language\", \"lnaugage\"),\n",
    "        (\"language\", \"lngauage\"),\n",
    "    ]\n",
    "    for s1, s2 in string_distance_examples:\n",
    "        print(f\"Edit distance btwn '{s1}' and '{s2}':\", edit_distance(s1, s2))\n",
    "        print(\n",
    "            f\"Edit dist with transpositions btwn '{s1}' and '{s2}':\",\n",
    "            edit_distance(s1, s2, transpositions=True),\n",
    "        )\n",
    "        print(f\"Jaro similarity btwn '{s1}' and '{s2}':\", jaro_similarity(s1, s2))\n",
    "        print(\n",
    "            f\"Jaro-Winkler similarity btwn '{s1}' and '{s2}':\",\n",
    "            jaro_winkler_similarity(s1, s2),\n",
    "        )\n",
    "        print(\n",
    "            f\"Jaro-Winkler distance btwn '{s1}' and '{s2}':\",\n",
    "            1 - jaro_winkler_similarity(s1, s2),\n",
    "        )\n",
    "    s1 = {1, 2, 3, 4}\n",
    "    s2 = {3, 4, 5}\n",
    "    print(\"s1:\", s1)\n",
    "    print(\"s2:\", s2)\n",
    "    print(\"Binary distance:\", binary_distance(s1, s2))\n",
    "    print(\"Jaccard distance:\", jaccard_distance(s1, s2))\n",
    "    print(\"MASI distance:\", masi_distance(s1, s2))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3113ff",
   "metadata": {},
   "source": [
    "I think the above implementation is an example for bottom up approach and a good example of dynamic programming where it included the usage of \"backtracking\", it started with using :\n",
    "\n",
    "The edit_distance_align first function initializes two matrices and stores the distances hence it is a bottom up approach\n",
    "The edit_distance function utilizes these bottom-up steps to compute the Levenshtein edit-distance between two strings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71432c",
   "metadata": {},
   "source": [
    "The Catalan numbers arise in many applications of combinatorial mathematics,\n",
    "including the counting of parse trees (Section 8.6). The series can be defined as\n",
    "follows: C0 = 1, and Cn+1 = Î£0..n (CiCn-i).\n",
    "a. Write a recursive function to compute nth Catalan number Cn.\n",
    "b. Now write another function that does this computation using dynamic programming.\n",
    "c. Use the timeit module to compare the performance of these functions as n\n",
    "increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59c0f0-6e5b-4ace-9c00-fab3a6de1c15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e67f7a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 2 5 14 42 132 429 1430 4862 "
     ]
    }
   ],
   "source": [
    "\n",
    "def Catalan(number):\n",
    "#Checking if the number is 1,since it doesnot need extra computation\n",
    "    if number <= 1:\n",
    "        return 1\n",
    "    # The basic formula for catalan is  catalan(i)*catalan(n-i-1)\n",
    "    res = 0\n",
    "    for i in range(n):\n",
    "        res += catalan(i) * catalan(n-i-1)\n",
    "        return res\n",
    " \n",
    " \n",
    "#Example for checking\n",
    "for i in range(10):\n",
    "    print(catalan(i), end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4cff3522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 2 5 14 42 132 429 1430 4862 "
     ]
    }
   ],
   "source": [
    "def catalan(n):\n",
    "    if (n == 0 or n == 1):\n",
    "        return 1\n",
    "    #creating the dp table\n",
    "    catalan = [0]*(n+1)\n",
    "        #Initializing and assigning first two values in the table\n",
    "    catalan[0] = 1\n",
    "    catalan[1] = 1\n",
    "    # using recursive formula\n",
    "    for i in range(2, n + 1):\n",
    "        for j in range(i):\n",
    "            catalan[i] += catalan[j] * catalan[i-j-1]\n",
    " \n",
    "    # Return last entry\n",
    "    return catalan[n]\n",
    " \n",
    "#Example to check:\n",
    "for i in range(10):\n",
    "    print(catalan(i), end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cdbc83e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 0\n",
      "Recursive time: 0.000283 seconds\n",
      "Dynamic Programming time: 0.000305 seconds\n",
      "\n",
      "n = 1\n",
      "Recursive time: 0.000314 seconds\n",
      "Dynamic Programming time: 0.000148 seconds\n",
      "\n",
      "n = 2\n",
      "Recursive time: 0.001099 seconds\n",
      "Dynamic Programming time: 0.001485 seconds\n",
      "\n",
      "n = 3\n",
      "Recursive time: 0.003175 seconds\n",
      "Dynamic Programming time: 0.001898 seconds\n",
      "\n",
      "n = 4\n",
      "Recursive time: 0.008667 seconds\n",
      "Dynamic Programming time: 0.002526 seconds\n",
      "\n",
      "n = 5\n",
      "Recursive time: 0.028980 seconds\n",
      "Dynamic Programming time: 0.003403 seconds\n",
      "\n",
      "n = 6\n",
      "Recursive time: 0.071600 seconds\n",
      "Dynamic Programming time: 0.004491 seconds\n",
      "\n",
      "n = 7\n",
      "Recursive time: 0.222343 seconds\n",
      "Dynamic Programming time: 0.005814 seconds\n",
      "\n",
      "n = 8\n",
      "Recursive time: 0.682142 seconds\n",
      "Dynamic Programming time: 0.010864 seconds\n",
      "\n",
      "n = 9\n",
      "Recursive time: 2.047167 seconds\n",
      "Dynamic Programming time: 0.009202 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Using timeit function to find time in seconds\n",
    "import timeit\n",
    "\n",
    "#First function\n",
    "\n",
    "def catalan_recursive(n):\n",
    "    if n <= 1:\n",
    "        return 1\n",
    "    res = 0\n",
    "    for i in range(n):\n",
    "        res += catalan_recursive(i) * catalan_recursive(n - i - 1)\n",
    "    return res\n",
    "\n",
    "#second fundtion\n",
    "    if n == 0 or n == 1:\n",
    "        return 1\n",
    "    catalan = [0] * (n + 1)\n",
    "    catalan[0] = 1\n",
    "    catalan[1] = 1\n",
    "    for i in range(2, n + 1):\n",
    "        for j in range(i):\n",
    "            catalan[i] += catalan[j] * catalan[i - j - 1]\n",
    "    return catalan[n]\n",
    "\n",
    "# Define a setup code to import necessary functions\n",
    "setup_code = \"\"\"\n",
    "from __main__ import catalan_recursive, catalan_dp\n",
    "\"\"\"\n",
    "\n",
    "# Measure the execution time for both functions for increasing values of n\n",
    "for n in range(10):\n",
    "    print(f\"n = {n}\")\n",
    "    recursive_time = timeit.timeit(\n",
    "        stmt=f\"catalan_recursive({n})\",\n",
    "        setup=setup_code,\n",
    "        number=1000  # Number of executions for accurate timing\n",
    "    )\n",
    "    print(f\"Recursive time: {recursive_time:.6f} seconds\")\n",
    "\n",
    "    dp_time = timeit.timeit(\n",
    "        stmt=f\"catalan_dp({n})\",\n",
    "        setup=setup_code,\n",
    "        number=1000  # Number of executions for accurate timing\n",
    "    )\n",
    "    print(f\"Dynamic Programming time: {dp_time:.6f} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b61b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
